{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 自动计算梯度  \n",
    "  \n",
    "|方法         |精确度         |是否支持任意代码      |备注               |   \n",
    "|----------  |:----------:  |:----------:       |-----:             |    \n",
    "|数值微分      |低            |是                 |实现琐碎            |  \n",
    "|符号微分      |高            |否                 |会构建一个完全不同的图 |\n",
    "|前向自动微分   |高            |是                 |基于二元树          |\n",
    "|反向自动微分   |高            |是                 |由Tensorflow实现    |\n",
    "\n",
    "<center>表2-1.自动计算梯度的主要方法</center>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TensorFlow是利用反向自动微分来实现自动计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载并整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape # 获取数据的行列数\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)),housing.data] # 为数据添加偏差项，即添加y=ax+b中的b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "scaler = StandardScaler().fit(housing_data_plus_bias)\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建计算图（一）\n",
    "# 数据转换为常量\n",
    "# 设置各种参数\n",
    "n_epochs = 1000\n",
    "global_learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\") # 数据标签\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")     # 参数\n",
    "y_pred =  tf.matmul(X, theta, name=\"prediction\")                          # 预测值\n",
    "error = y_pred-y                                                          # 误差\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                        # 均方误差(成本函数)\n",
    "gradient = tf.gradients(mse, [theta])[0]                                  # 使用反向自动微分计算梯度\n",
    "training_op = tf.assign(theta, theta-global_learning_rate*gradient)       # 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用优化器 \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在TensorFLow中不仅可以自动求得梯度，还可以调用各种优化器对参数$\\theta$进行优化。调用优化器时，只需在构建计算图步骤时更改对`training_op=...`赋值的语句即可，详情如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建计算图（二）\n",
    "n_epochs = 1000\n",
    "global_learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\") # 数据标签\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")     # 参数\n",
    "y_pred =  tf.matmul(X, theta, name=\"prediction\")                          # 预测值\n",
    "error = y_pred-y                                                          # 误差\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                        # 均方误差(成本函数)\n",
    "# gradient = tf.gradients(mse, [theta])[0]                                  # 使用反向自动微分计算梯度\n",
    "# 调用特定的优化器对参数进行优化\n",
    "## 定义优化器(梯度下降)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate = global_learning_rate)\n",
    "## 定义优化器（动量）\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = global_learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(mse)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建会话，运行计算图，获得并观察结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 MSE= 10.322025\n",
      "Epoch: 100 MSE= 4.806012\n",
      "Epoch: 200 MSE= 4.803412\n",
      "Epoch: 300 MSE= 4.803272\n",
      "Epoch: 400 MSE= 4.8032565\n",
      "Epoch: 500 MSE= 4.8032546\n",
      "Epoch: 600 MSE= 4.8032537\n",
      "Epoch: 700 MSE= 4.8032537\n",
      "Epoch: 800 MSE= 4.8032537\n",
      "Epoch: 900 MSE= 4.8032537\n",
      "The best theta is [[ 0.18474627]\n",
      " [ 0.82961535]\n",
      " [ 0.118751  ]\n",
      " [-0.26551932]\n",
      " [ 0.30569002]\n",
      " [-0.00450321]\n",
      " [-0.03932611]\n",
      " [-0.8998956 ]\n",
      " [-0.87055033]]\n",
      "The running time: 0:00:00.346600\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()                                  # 添加初始化节点\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):                                       # 逐步训练\n",
    "        if epoch%100==0:\n",
    "            print(\"Epoch:\", epoch, \"MSE=\", mse.eval())                    # 每一步均方误差\n",
    "        sess.run(training_op)                                             # 执行每一步训练，更新梯度\n",
    "        \n",
    "    best_theta = theta.eval()                                             # 训练完毕，返回最佳参数\n",
    "    print(\"The best theta is\", best_theta)\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"The running time:\", (endtime - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 给训练算法提供数据\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要实现最小批量梯度下降算法，需要每次训练时用小批量替换输入数据X和y。可以添加一个占位符节点执行该替换操作。它不进行任何计算，只在运行时输出需要输出的值。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建计算图（三），定义占位符节点，设置各种参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "n_batches= int(np.ceil(m/batch_size))\n",
    "global_learning_rate = 0.01\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")     # * 参数 seed=42\n",
    "y_pred =  tf.matmul(X, theta, name=\"prediction\")                          # 预测值\n",
    "error = y_pred-y                                                          # 误差\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                        # 均方误差(成本函数)\n",
    "# 手工计算梯度\n",
    "## 使用反向自动微分计算梯度\n",
    "# * gradient = tf.gradients(mse, [theta])[0]                                  \n",
    "# 调用特定的优化器对参数进行优化\n",
    "## 定义优化器(梯度下降)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = global_learning_rate)\n",
    "## * 定义优化器（动量）由于使用批量梯度下降算法，所以不可以使用动量优化器，否则会报错\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate = global_learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(mse)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建会话，运行计算图，获得并观察结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best theta is [[ 0.4020946 ]\n",
      " [ 0.8377844 ]\n",
      " [ 0.10645497]\n",
      " [-0.25947902]\n",
      " [ 0.29196444]\n",
      " [ 0.00181689]\n",
      " [ 0.2128084 ]\n",
      " [-0.89034677]\n",
      " [-0.85242176]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()                                  # 添加初始化节点\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    # ？？？？\n",
    "    np.random.seed(epoch * n_batches + batch_index) \n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    best_theta = theta.eval()\n",
    "    print(\"The best theta is\", best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
